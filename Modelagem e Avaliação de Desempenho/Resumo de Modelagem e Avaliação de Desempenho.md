# 1. Defini√ß√µes

**Processo estoc√°stico**: √© um conjunto de vari√°veis aleat√≥rias $X(t),t \in T$ que descreve a evolu√ß√£o de um sistema ao longo do tempo sob influ√™ncia do acaso
	Cada $X(t)$ representa o estado aleat√≥rio do sistema no instante $t$

**Espa√ßo Amostral** $(\Omega)$: Conjunto de todas as sa√≠das poss√≠veis de um experimento aleat√≥rio.

**Evento**:  √© um subconjunto qualquer de $\Omega$

**Probabilidade Sim√©trica**: √© assumir que as sa√≠das poss√≠veis (Experimento ou $\Omega$) s√£o equiprov√°veis (tem as mesma probabilidade)

**Probabilidade Frequencista**: a probabilidade $P(E)$ de um evento $E$ √© dado pela raz√£o entre o n¬∫ de resultados favor√°veis e o n¬∫ total de resultados
$$
P(E) = \lim_{n\rightarrow \inf} \frac{\text{N¬∫ de ocorr√™ncias E}}{n}
$$

**Probabilidade Condicional**: Para eventos A e B, a probabilidade condicional de _A dado B_ √© definida como
$$
\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}
$$

**Probabilidade Conjunta** (Depend√™ncia):
$$
\mathbb{P}(A\cap B)=\mathbb{P}(A|B)\cdot \mathbb{P}(B)
$$
**Independ√™ncia**: Dois eventos A e B s√£o independentes se
$$
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)
$$
**Teorema Probabilidade Total**: afirma que, se um conjunto de eventos $( B_1, B_2, \dots, B_n )$ forma uma **parti√ß√£o** do espa√ßo amostral (isto √©, s√£o mutuamente exclusivos e cobrem todo o espa√ßo), ent√£o a probabilidade de um evento $( A )$ pode ser expressa como:
$$
P(A) = \sum_{i=1}^{n} P(A \mid B_i) \, P(B_i)
$$
**Regra de Bayes**: Se A e B s√£o eventos com probabilidade positiva, ent√£o
$$P(A|B) = \frac{(P(A)‚ãÖP(B|A))}{P(B)}$$
obs: $P(A‚à©B)=P(A)‚ãÖ(B|A)$
$P(A|B)+P(A|B^c)=1$

Eventos mutuamente exclusivos: $P(A ‚à™ B) = P(A) + P(B)$

Complemento:
- $P(A^c)=1-P(A)$
- $(A ‚à© B)^c = A^c ‚à™ B^c$
- $(A ‚à™ B)^c = A^c ‚à© B^c$

## Fun√ß√£o de Massa de Probabilidade (PMF)
Aplica-se a **vari√°veis aleat√≥rias discretas**.  
Ela fornece a probabilidade de a vari√°vel assumir um valor espec√≠fico:
$$
P(X = x) = f(x)
$$
Deve satisfazer:
$$0 \le f(x) \le 1 \quad \text{e} \quad \sum_x f(x) = 1$$
## Fun√ß√£o de Densidade de Probabilidade (PDF)
Aplica-se a **vari√°veis aleat√≥rias cont√≠nuas**.  
Ela descreve a **densidade de probabilidade**, e n√£o a probabilidade direta.  
A probabilidade de $X$ estar em um intervalo $[a,b]$ √©:
$$
P(a \le X \le b) = \int_a^b f(x)\,dx
$$
Deve satisfazer:
$$
f(x) \ge 0 \quad \text{e} \quad \int_{-\infty}^{\infty} f(x)\,dx = 1
$$
## Linearidade da Esperan√ßa
A **linearidade da esperan√ßa** afirma que a **esperan√ßa (ou valor esperado)** de uma soma de vari√°veis aleat√≥rias √© igual √† soma das esperan√ßas individuais, **independentemente de haver depend√™ncia entre elas**:
$$
E[aX + bY] = aE[X] + bE[Y] \quad E[X,Y]= E[X]+E[Y]
$$
ou, mais geralmente,
$$
E\!\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} E[X_i]
$$
# 2. Vari√°veis Aleat√≥rias Discretas
## Modelo Bernoulli
> Sucesso ou Fracasso

$$
X \sim Ber(p) \hspace{1cm}(0 < p <1)
$$
$$
\mathbb{p}_X(x)=p^x(1-p)^{1-x} \hspace{20px}\text{para }x\in \{0,1\}
$$
- $E[X] = P$
- $Var(X) = P(1-P)$
$$
\begin{matrix}
\text{PMF:} && \text{CDF:} \\ 
P(X = x) = \begin{cases} p, & x = 1 \\ 1 - p, & x = 0 \end{cases} &&
F(x) = \begin{cases} 0, & x < 0 \\ 1 - p, & 0 \le x < 1 \\ 1, & x \ge 1 \end{cases}
\end{matrix}
$$
## Modelo Binomial
$X \sim Bin(n,p)$

> Chama-se de experimento binomial ao experimento que 
> - consiste em n ensaios de Bernoulli
> - cujo ensaios s√£o independentes, e
> - para qual a probabilidade de sucessos em casa ensaio √© sempre igual a p $(0<p<1)$

- PMF
$$
\begin{matrix} p_X=\mathbb{P}(X=x)= \dbinom{n}{x}\centerdot p^x\centerdot(1-p)^{n-x},\\ \dbinom{n}{x}= \frac {n!}{x!(n-x)!} & x \in \{0,1,2,...,n\}\end{matrix}
$$
- CDF
$$
F(k)=\sum\limits^k_{i=0} \dbinom{n}{i}\centerdot p^{i\centerdot(1-p)^{n-i}}= \sum\limits^k_{y \le k}p_x^{(y)}
$$
- $E[X] = n \centerdot p$
- $Var(X)=n\centerdot p \centerdot (1-p)$
## Modelo Geom√©trico
$X \sim Geom(p)$
> N√∫mero de repeti√ß√µes de um ensaio de Bernoulli com probabilidade de sucesso $(0<p<1)$at√© ocorrer o primeiro sucesso

- PMF:
$$
\mathbb{P}(X=x)=p\cdot(1 ‚àí p)^x \hspace{1cm},x \in \mathbb{N}
$$
- CDF:
$$
F(k)=1-(1-p)^k
$$
- $E[X]= \frac{1}{p}$
- $Var(X)= \frac{1-P}{p^2}$
## Modelo Poisson
$X \sim Poi(\lambda)$
> N¬∫ de eventos que ocorrem em um intervalo de tempo ou espa√ßo

- PMF:
$$
\begin{matrix} p(x) = e^{-\lambda} \centerdot \frac{\lambda^x}{x!}, & x=\{0,1,2,..\} \end{matrix}
$$
- CDF:
$$
F(k)=\sum\limits^k_{i=0}p(x)
$$

$E[X] = Var(x) = \lambda$

## Uniforme Cont√≠nua

> **Descri√ß√£o:** Todos os valores em $[a,b]$ igualmente prov√°veis.

- **PDF:**
$$
f(x) = \begin{cases} \frac{1}{b-a}, & a \le x \le b \\ 0, & \text{caso contr√°rio} \end{cases}
$$    
- **CDF:**
    $$
  F(x) = \begin{cases} 0, & x < a \\ \frac{x - a}{b - a}, & a \le x < b \\ 1, & x \ge b \end{cases}  
$$
## Normal (Gaussiana)

 > **Descri√ß√£o:** Distribui√ß√£o sim√©trica em torno da m√©dia $\mu$.
    
- **PDF:**
    $$
  f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}  
$$
- **CDF:**
    $$
  F(x) = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(t - \mu)^2}{2\sigma^2}}\, dt  
$$(sem forma fechada; usa tabelas ou fun√ß√µes computacionais).
# 3. Rela√ß√£o Poisson $\times$ Exponencial (Processo de Poisson)

A **distribui√ß√£o de Poisson** e a **distribui√ß√£o Exponencial** est√£o intimamente ligadas ‚Äî elas descrevem **dois lados do mesmo processo estoc√°stico**, o **Processo de Poisson**.

## Processo de Poisson

Modela o n√∫mero de ocorr√™ncias de um evento em um intervalo de tempo:  
$$
P(N(t) = k) = \frac{e^{-\lambda t} (\lambda t)^k}{k!}  
$$
onde \( $\lambda$ \) √© a **taxa m√©dia de eventos por unidade de tempo**.

üëâ Assim, \( $N(t)$ \) segue **distribui√ß√£o de Poisson**.
## Tempo entre eventos ‚Üí Exponencial

O **tempo entre dois eventos consecutivos**, chamado de _tempo de interchegada_, segue uma **distribui√ß√£o Exponencial**:  
$$
f_T(t) = \lambda e^{-\lambda t}, \quad t \ge 0  

$$
com m√©dia \( $E[T] = 1/\lambda$ \).

Portanto:
- Poisson ‚Üí **quantos eventos ocorrem em um tempo fixo**.
- Exponencial ‚Üí **quanto tempo at√© o pr√≥ximo evento**.
## Rela√ß√£o formal

Se os tempos entre eventos \( $T_1, T_2, \dots$ \) s√£o independentes e Exponenciais ($\lambda$ ),  
ent√£o o n√∫mero total de eventos at√© o tempo \( t \):  
$$
N(t) = \max{ n : T_1 + T_2 + \cdots + T_n \le t }  

$$
segue uma **distribui√ß√£o de Poisson( $\lambda t$ )**.

E reciprocamente, se ( $N(t)$ ) √© um processo de Poisson, ent√£o os tempos entre eventos s√£o **Exponenciais( $\lambda$ )**.

| Aspecto                         | Distribui√ß√£o       | Interpreta√ß√£o               |
| ------------------------------- | ------------------ | --------------------------- |
| N√∫mero de eventos em tempo fixo | **Poisson(Œªt)**    | Contagem de ocorr√™ncias     |
| Tempo entre eventos             | **Exponencial(Œª)** | Intervalo entre ocorr√™ncias |
# 4. Gera√ß√£o de Amostras Aleat√≥rias